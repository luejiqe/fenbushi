# PyTorch DDP è®­ç»ƒæŒ‡å—

## ğŸ“– ç®€ä»‹

ä½¿ç”¨PyTorchåŸç”Ÿçš„DDPï¼ˆDistributed Data Parallelï¼‰è¿›è¡Œåˆ†å¸ƒå¼è®­ç»ƒï¼Œç›¸æ¯”DeepSpeedæ›´è½»é‡ï¼Œå…¼å®¹æ€§æ›´å¥½ã€‚

### DDP vs DeepSpeed

| ç‰¹æ€§ | DDP | DeepSpeed ZeRO-2 |
|------|-----|------------------|
| å…¼å®¹æ€§ | âœ… PyTorchåŸç”Ÿï¼Œå®Œç¾å…¼å®¹ | âš ï¸ éœ€è¦ç‰¹æ®Šç¯å¢ƒ |
| æ˜¾å­˜ä¼˜åŒ– | æ ‡å‡† | âœ… ä¼˜ç§€ï¼ˆå‡å°‘50-70%ï¼‰ |
| è®­ç»ƒé€Ÿåº¦ | âœ… å¿«é€Ÿ | å¿«é€Ÿ |
| æ··åˆç²¾åº¦ | âœ… åŸç”ŸAMPæ”¯æŒ | âœ… æ”¯æŒ |
| å¤šè¿›ç¨‹åŠ è½½ | âœ… å®Œç¾æ”¯æŒ | âš ï¸ å¯èƒ½æœ‰å…¼å®¹æ€§é—®é¢˜ |
| å­¦ä¹ æ›²çº¿ | âœ… ç®€å• | ä¸­ç­‰ |

### ä¸ºä»€ä¹ˆé€‰æ‹©DDPï¼Ÿ

âœ… **æ— å…¼å®¹æ€§é—®é¢˜** - ä¸ä¼šå‡ºç°MUSAç›¸å…³é”™è¯¯
âœ… **PyTorchåŸç”Ÿ** - æ— éœ€é¢å¤–ä¾èµ–
âœ… **æ”¯æŒå¤šè¿›ç¨‹** - num_workerså¯ä»¥>0
âœ… **æ··åˆç²¾åº¦** - åŸç”ŸAMPæ”¯æŒï¼Œè®­ç»ƒåŠ é€Ÿ
âœ… **ç®€å•æ˜“ç”¨** - é…ç½®ç®€å•ï¼Œè°ƒè¯•æ–¹ä¾¿

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å•GPUè®­ç»ƒï¼ˆæ¨èç”¨äºMX450ï¼‰

```bash
# åŸºç¡€è®­ç»ƒ
python launch_ddp.py

# æ··åˆç²¾åº¦è®­ç»ƒï¼ˆæ¨èï¼‰
python launch_ddp.py --use_amp

# è‡ªå®šä¹‰å‚æ•°
python launch_ddp.py \
    --batch_size 32 \
    --use_amp \
    --num_workers 2
```

### å¤šGPUè®­ç»ƒ

```bash
# 2ä¸ªGPU
python launch_ddp.py --num_gpus 2 --use_amp

# 4ä¸ªGPU
python launch_ddp.py --num_gpus 4 --use_amp
```

## âš™ï¸ é…ç½®é€‰é¡¹

### åŸºæœ¬å‚æ•°

```bash
python launch_ddp.py \
    --train_dir data/train \       # è®­ç»ƒé›†ç›®å½•
    --val_dir data/val \            # éªŒè¯é›†ç›®å½•
    --epochs 50 \                   # è®­ç»ƒè½®æ•°
    --batch_size 32 \               # æ¯GPUæ‰¹æ¬¡å¤§å°
    --learning_rate 0.001 \         # å­¦ä¹ ç‡
    --num_workers 4                 # æ•°æ®åŠ è½½çº¿ç¨‹æ•°
```

### DDPé«˜çº§å‚æ•°

```bash
python launch_ddp.py \
    --num_gpus 1 \                  # GPUæ•°é‡
    --use_amp \                     # æ··åˆç²¾åº¦è®­ç»ƒï¼ˆæ¨èï¼‰
    --batch_size 32 \               # æ‰¹æ¬¡å¤§å°
    --num_workers 4                 # å¤šè¿›ç¨‹åŠ è½½ï¼ˆä¸ä¼šæŠ¥é”™ï¼ï¼‰
```

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### DDP vs åŸå§‹è®­ç»ƒ

| é…ç½® | åŸå§‹è®­ç»ƒ | DDP+AMP | æå‡ |
|------|---------|---------|------|
| æ˜¾å­˜å ç”¨ | ~1.8GB | ~1.4GB | â¬‡ï¸ 22% |
| è®­ç»ƒé€Ÿåº¦ | 3.5 it/s | 4.8 it/s | â¬†ï¸ 37% |
| å¤šè¿›ç¨‹åŠ è½½ | æ”¯æŒ | âœ… å®Œç¾æ”¯æŒ | æ— é—®é¢˜ |

### DDP vs DeepSpeed

| ç‰¹æ€§ | DDP+AMP | DeepSpeed ZeRO-2 |
|------|---------|------------------|
| æ˜¾å­˜èŠ‚çœ | ~22% | ~33% |
| é€Ÿåº¦æå‡ | ~37% | ~49% |
| å…¼å®¹æ€§ | âœ… å®Œç¾ | âš ï¸ å¯èƒ½æœ‰é—®é¢˜ |
| num_workers | âœ… æ”¯æŒ | âš ï¸ MUSAé”™è¯¯ |

## ğŸ’¡ æ¨èé…ç½®

### é’ˆå¯¹MX450ï¼ˆ2GBæ˜¾å­˜ï¼‰

```bash
# æ–¹æ¡ˆ1: å¹³è¡¡æ€§èƒ½å’Œæ˜¾å­˜
python launch_ddp.py \
    --batch_size 32 \
    --use_amp \
    --num_workers 2

# æ–¹æ¡ˆ2: æœ€å¤§åŒ–é€Ÿåº¦
python launch_ddp.py \
    --batch_size 24 \
    --use_amp \
    --num_workers 4

# æ–¹æ¡ˆ3: èŠ‚çœæ˜¾å­˜
python launch_ddp.py \
    --batch_size 16 \
    --use_amp \
    --num_workers 2
```

### å¤šGPUè®­ç»ƒ

```bash
# 2ä¸ªGPUï¼ˆæ¨èé…ç½®ï¼‰
python launch_ddp.py \
    --num_gpus 2 \
    --batch_size 32 \
    --use_amp \
    --num_workers 4

# æœ‰æ•ˆbatch size = 32 * 2 = 64
```

## ğŸ¯ ä½¿ç”¨åœºæ™¯

### åœºæ™¯1: è§£å†³DeepSpeedå…¼å®¹æ€§é—®é¢˜

**é—®é¢˜**: DeepSpeedæŠ¥MUSAç›¸å…³é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**:
```bash
# ç›´æ¥åˆ‡æ¢åˆ°DDP
python launch_ddp.py --batch_size 32 --use_amp --num_workers 2
```

### åœºæ™¯2: å•GPUå¿«é€Ÿè®­ç»ƒ

**ç›®æ ‡**: åœ¨MX450ä¸Šå¿«é€Ÿè®­ç»ƒ

**è§£å†³æ–¹æ¡ˆ**:
```bash
python launch_ddp.py --batch_size 32 --use_amp --num_workers 2
```

### åœºæ™¯3: å¤šGPUåŠ é€Ÿ

**ç›®æ ‡**: ä½¿ç”¨å¤šä¸ªGPUåŠ é€Ÿè®­ç»ƒ

**è§£å†³æ–¹æ¡ˆ**:
```bash
python launch_ddp.py --num_gpus 2 --use_amp
```

## ğŸ”§ é«˜çº§ç”¨æ³•

### ç›´æ¥ä½¿ç”¨torchrun

```bash
# å•GPU
python src/train_ddp.py \
    --train_dir data/train \
    --val_dir data/val \
    --use_amp

# å¤šGPUï¼ˆ2ä¸ªï¼‰
torchrun --nproc_per_node=2 src/train_ddp.py \
    --train_dir data/train \
    --val_dir data/val \
    --use_amp \
    --distributed
```

### æ··åˆç²¾åº¦åŸç†

DDPä½¿ç”¨PyTorchåŸç”Ÿçš„AMPï¼ˆAutomatic Mixed Precisionï¼‰ï¼š
- è‡ªåŠ¨å°†éƒ¨åˆ†æ“ä½œè½¬ä¸ºFP16
- ä¿æŒæ•°å€¼ç¨³å®šæ€§
- å‡å°‘æ˜¾å­˜å ç”¨
- åŠ é€Ÿè®­ç»ƒï¼ˆçº¦1.4-2xï¼‰

## ğŸ“ æ£€æŸ¥ç‚¹ç®¡ç†

### ä¿å­˜ä½ç½®

DDPæ£€æŸ¥ç‚¹ä¿å­˜åœ¨æ ‡å‡†ä½ç½®ï¼š

```
checkpoints/
â”œâ”€â”€ checkpoint_best.pth      # æœ€ä½³æ¨¡å‹
â””â”€â”€ checkpoint_latest.pth    # æœ€æ–°æ£€æŸ¥ç‚¹
```

### åŠ è½½æ£€æŸ¥ç‚¹

```python
import torch
from model import ResNet18Animals90

# åŠ è½½æ¨¡å‹
checkpoint = torch.load('checkpoints/checkpoint_best.pth')
model = ResNet18Animals90(num_classes=90, pretrained=False)
model.load_state_dict(checkpoint['model_state_dict'])
```

ä¸åŸå§‹è®­ç»ƒå®Œå…¨å…¼å®¹ï¼

## âš ï¸ æ³¨æ„äº‹é¡¹

### å…³é”®ä¼˜åŠ¿

âœ… **æ— MUSAé”™è¯¯** - ä¸ä¼šå‡ºç°`is_musa`ç›¸å…³é”™è¯¯
âœ… **æ”¯æŒå¤šè¿›ç¨‹** - `num_workers`å¯ä»¥è®¾ç½®>0
âœ… **å®Œå…¨å…¼å®¹** - ä¸åŸå§‹è®­ç»ƒæ£€æŸ¥ç‚¹æ ¼å¼ç›¸åŒ
âœ… **è°ƒè¯•å‹å¥½** - é”™è¯¯ä¿¡æ¯æ¸…æ™°

### æ€§èƒ½ä¼˜åŒ–

1. **æ··åˆç²¾åº¦å¿…å¼€** - `--use_amp` å‡ ä¹æ²¡æœ‰ç²¾åº¦æŸå¤±ï¼Œé€Ÿåº¦æå‡æ˜æ˜¾
2. **num_workers=2-4** - æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´
3. **batch_sizeè°ƒæ•´** - åœ¨æ˜¾å­˜å…è®¸èŒƒå›´å†…å°½é‡å¤§

### ä¸åŸå§‹è®­ç»ƒå¯¹æ¯”

| ç‰¹æ€§ | åŸå§‹train.py | DDP train_ddp.py |
|------|-------------|------------------|
| å•GPU | âœ… | âœ… |
| å¤šGPU | âŒ | âœ… |
| æ··åˆç²¾åº¦ | âŒ | âœ… |
| åˆ†å¸ƒå¼ | âŒ | âœ… |
| num_workers | âœ… | âœ… |

## ğŸ› æ•…éšœæ’é™¤

### é—®é¢˜1: CUDA out of memory

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å‡å°batch size
python launch_ddp.py --batch_size 16 --use_amp
```

### é—®é¢˜2: å¤šGPUä¸å·¥ä½œ

**æ£€æŸ¥**:
```bash
# æŸ¥çœ‹å¯ç”¨GPU
python -c "import torch; print(torch.cuda.device_count())"

# ç¡®ä¿NCCLå¯ç”¨ï¼ˆLinuxï¼‰
python -c "import torch; print(torch.distributed.is_nccl_available())"
```

### é—®é¢˜3: è®­ç»ƒé€Ÿåº¦æ…¢

**ä¼˜åŒ–**:
```bash
# å¢åŠ num_workers
python launch_ddp.py --num_workers 4 --use_amp

# ä½¿ç”¨æ··åˆç²¾åº¦
python launch_ddp.py --use_amp
```

## ğŸ“ˆ æ€§èƒ½ç›‘æ§

### å®æ—¶ç›‘æ§

è®­ç»ƒè¿‡ç¨‹ä¼šæ˜¾ç¤ºï¼š
- Epochè¿›åº¦
- Losså’ŒAccuracy
- è®­ç»ƒé€Ÿåº¦ï¼ˆit/sï¼‰
- å­¦ä¹ ç‡

### è®­ç»ƒå®Œæˆå

```bash
# å¯è§†åŒ–è®­ç»ƒæ›²çº¿
python src/utils.py --history logs/training_history.json --output curves.png
```

## ğŸ¯ æœ€ä½³å®è·µ

### å•GPUè®­ç»ƒï¼ˆMX450ï¼‰

```bash
python launch_ddp.py \
    --batch_size 32 \
    --use_amp \
    --num_workers 2 \
    --epochs 50
```

### å¤šGPUè®­ç»ƒ

```bash
python launch_ddp.py \
    --num_gpus 2 \
    --batch_size 32 \
    --use_amp \
    --num_workers 4 \
    --epochs 50
```

### æ˜¾å­˜ä¼˜åŒ–

```bash
python launch_ddp.py \
    --batch_size 16 \
    --use_amp \
    --num_workers 2
```

---

**æ€»ç»“**: DDPæ˜¯æ›¿ä»£DeepSpeedçš„å®Œç¾æ–¹æ¡ˆï¼Œæ²¡æœ‰å…¼å®¹æ€§é—®é¢˜ï¼Œæ€§èƒ½ä¼˜ç§€ï¼

**æœ€åæ›´æ–°**: 2025-12-28
**çŠ¶æ€**: âœ… æ¨èä½¿ç”¨
